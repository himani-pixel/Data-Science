{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d382114-892f-405b-92a2-617172272bee",
   "metadata": {},
   "source": [
    "# Deep Learning – Common Terminology\n",
    "\n",
    "Before diving into neural networks, it’s important to understand some **core terms** that frequently appear in deep learning discussions.  \n",
    "These concepts are explained visually and intuitively in the reference video, and the notes below follow the same flow.\n",
    "\n",
    "---\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "A **perceptron** is the **simplest form of a neural network** — it consists of just **one neuron**.\n",
    "\n",
    "It:\n",
    "- Takes multiple **inputs**\n",
    "- Multiplies them with **weights**\n",
    "- Adds a **bias**\n",
    "- Passes the result through an **activation function**\n",
    "\n",
    "The perceptron then makes a **binary decision**, such as classifying an output as **0 or 1**, similar to the demonstration shown in the video.\n",
    "\n",
    "---\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "A **neural network** is a collection of **interconnected perceptrons (neurons)** organized into layers.\n",
    "\n",
    "Each layer:\n",
    "- Applies **weights**\n",
    "- Adds **biases**\n",
    "- Uses **activation functions** to transform inputs\n",
    "\n",
    "A **deep neural network** contains **multiple hidden layers**, allowing it to learn and model **complex patterns**, as illustrated layer-by-layer in the video.\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "**Hyperparameters** are values that are **set before training begins**.  \n",
    "They are **not learned from the data**, but they strongly influence how learning happens.\n",
    "\n",
    "Common hyperparameters include:\n",
    "\n",
    "- **Learning rate** – Controls how much weights are updated\n",
    "- **Number of epochs** – How many times the full dataset is used\n",
    "- **Batch size** – Number of samples processed before weight updates\n",
    "- **Number of layers or neurons** – Defines the network’s structure\n",
    "\n",
    "These are tuned experimentally, just like shown in the training walkthrough in the video.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Rate (η)\n",
    "\n",
    "The **learning rate (η)** controls **how much the model adjusts its weights** after each training step.\n",
    "\n",
    "- Too **high** → model may overshoot the optimal solution\n",
    "- Too **low** → training becomes very slow\n",
    "\n",
    "The video visually demonstrates how different learning rates affect convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Training\n",
    "\n",
    "**Training** is the phase where the model **learns from data**.\n",
    "\n",
    "During training:\n",
    "- The model makes predictions\n",
    "- Errors are calculated by comparing predictions with actual outputs\n",
    "- Weights are updated to reduce these errors\n",
    "\n",
    "This iterative learning process is shown clearly in the training loop explained in the video.\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "**Backpropagation** is the algorithm used to **update weights** in a neural network.\n",
    "\n",
    "It works by:\n",
    "- Computing the **error (loss)**\n",
    "- Calculating **gradients** using the **chain rule**\n",
    "- Propagating the error **backwards** through the network\n",
    "\n",
    "This allows the model to **learn from its mistakes**, exactly as visualized step-by-step in the video.\n",
    "\n",
    "---\n",
    "\n",
    "## Inference\n",
    "\n",
    "**Inference** is the phase where a **trained model** is used to make predictions on **new, unseen data**.\n",
    "\n",
    "- No learning happens here\n",
    "- Weights remain fixed\n",
    "- The model only performs forward computation\n",
    "\n",
    "The video highlights this difference between training and inference clearly.\n",
    "\n",
    "---\n",
    "\n",
    "## Activation Function\n",
    "\n",
    "An **activation function** introduces **non-linearity** into the network, enabling it to learn complex relationships.\n",
    "\n",
    "Common activation functions include:\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**\n",
    "- **Sigmoid** – outputs values between 0 and 1\n",
    "- **Tanh** – outputs values between -1 and 1\n",
    "\n",
    "While early perceptrons used a **step function**, modern neural networks primarily use **ReLU**, as shown in the video examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Epoch\n",
    "\n",
    "An **epoch** means **one complete pass** through the entire training dataset.\n",
    "\n",
    "- Models are trained for **multiple epochs**\n",
    "- Each epoch helps refine the learned patterns\n",
    "- More epochs generally improve learning (up to a point)\n",
    "\n",
    "This repeated learning cycle is demonstrated clearly in the training timeline shown in the video."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
